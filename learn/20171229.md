mbd, cant type Chinese cause I am fucking in Linux....

watever, ill just check it out...

今天本来打算仿照morvan的基于keras的classifier程序做一个分类器，X是自然数的拆分表达，例如123=[1 2 3]，Y是X关于某个自然数的余数，但是恍然发现，就连Y=X mod 2的自动求解都很有困难，更不提Y=X mod 3,。。。。不过呢，Y = X mod 10这种傻逼式的求余对classifier还是很easy的，毕竟，最后一位的权重是100%。。。

后来想了想，这个求余对nn系统太难了，因为求余可能是某种超高维度的操作，这些遐想带我驶入另一个领域：

人脑天生只能做纯粹的降维运算，换言之，即是信号抽象，人脑对于精确的regression可能完全无能为力。。。

这些都是我猜的。。。

但是基于这些猜测，我意识到编码的重要性。如果要做classification，原始数据编码必须具有极强的稀疏性，否则，在一个极大的空间内，各种数据杂糅在一起，对nn的分类是个大坑，我们应该在classification方法中将数据尽可能稀疏的，平均的分布在整个定义域空间内，对应的值域空间也应该如此。

所以，在learn_numbers.py这个作业中，我先把X做了一个categorization，效果立竿见影，未做之前，correctness percentage在90%左右，无法提高（可能因为定义域空间的狭小区域各个不同变量相互挤兑），做了之后，只需要极小的样本极少的迭代次数，甚至极少的神经元，在cpu上就可以达到100%的正确率，而且mse减小至1e-3的水平！！！

我不想写了，我想睡觉！